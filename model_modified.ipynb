{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sinagho/W-Mamba/blob/main/model_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk_mCoIO5zsU"
      },
      "source": [
        "# Libs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip uninstall mamba-ssm causal-conv1d\n",
        "!pip install causal-conv1d && pip install mamba-ssm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wt4VECBt6UCa",
        "outputId": "9286859f-4214-4fa4-bd1d-4b6003eb2b9a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.4.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.0%2Bcu121-cp310-cp310-linux_x86_64.whl (799.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.1/799.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.19.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.0%2Bcu121-cp310-cp310-linux_x86_64.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.4.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.0%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.0.0 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.19.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.19.0) (11.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu121\n",
            "    Uninstalling torchvision-0.20.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.5.1+cu121\n",
            "    Uninstalling torchaudio-2.5.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.5.1+cu121\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0+cu121 torchaudio-2.4.0+cu121 torchvision-0.19.0+cu121 triton-3.0.0\n",
            "\u001b[33mWARNING: Skipping mamba-ssm as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping causal-conv1d as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting causal-conv1d\n",
            "  Downloading causal_conv1d-1.5.0.post8.tar.gz (9.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from causal-conv1d) (2.4.0+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from causal-conv1d) (24.2)\n",
            "Collecting ninja (from causal-conv1d)\n",
            "  Downloading ninja-1.11.1.2-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->causal-conv1d) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->causal-conv1d) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->causal-conv1d) (1.3.0)\n",
            "Downloading ninja-1.11.1.2-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: causal-conv1d\n",
            "  Building wheel for causal-conv1d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for causal-conv1d: filename=causal_conv1d-1.5.0.post8-cp310-cp310-linux_x86_64.whl size=103955196 sha256=0ca2375ba7fb0c5022696dc3086c284e0eb1a7b33cf57aced23ec726c3d2b0a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/ef/0a/d9abf869acdd5fc07f403f4d8dd9db650cd66e81528a907941\n",
            "Successfully built causal-conv1d\n",
            "Installing collected packages: ninja, causal-conv1d\n",
            "Successfully installed causal-conv1d-1.5.0.post8 ninja-1.11.1.2\n",
            "Collecting mamba-ssm\n",
            "  Downloading mamba_ssm-2.2.4.tar.gz (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.4.0+cu121)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (1.11.1.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (0.8.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (4.46.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (24.2)\n",
            "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mamba-ssm) (12.6.85)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (4.66.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba-ssm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mamba-ssm) (1.3.0)\n",
            "Building wheels for collected packages: mamba-ssm\n",
            "  Building wheel for mamba-ssm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba-ssm: filename=mamba_ssm-2.2.4-cp310-cp310-linux_x86_64.whl size=323653202 sha256=c6edff068928b4ceacc6e820a163908c02294fe01dbcc05e95ed4530a5f81e77\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/af/c7/fb77bfcd94bd3e052545033449d8c47dc97222d79c39c5bc67\n",
            "Successfully built mamba-ssm\n",
            "Installing collected packages: mamba-ssm\n",
            "Successfully installed mamba-ssm-2.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtUIZAUk5zsV",
        "outputId": "4306aece-6691-4cab-b673-c30e69f0daaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install triton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkqwOMqm5zsW",
        "outputId": "dc11c1e2-f962-4c49-b8a8-526f46b05459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (24.2)\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm==0.4.12) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.4.12) (0.19.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4->timm==0.4.12) (12.6.85)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.12) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.12) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->timm==0.4.12) (1.3.0)\n",
            "Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.12\n",
            "    Uninstalling timm-1.0.12:\n",
            "      Successfully uninstalled timm-1.0.12\n",
            "Successfully installed timm-0.4.12\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (8.3.4)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.2.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.5.0)\n",
            "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest) (2.2.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs) (6.0.2)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: yacs\n",
            "Successfully installed yacs-0.1.8\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install packaging\n",
        "!pip install timm==0.4.12\n",
        "!pip install pytest\n",
        "!pip install chardet\n",
        "!pip install yacs\n",
        "!pip install termcolor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsCHw9Os5zsW"
      },
      "source": [
        "# Base Codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gwKSX-KX5zsW"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "from functools import partial\n",
        "from typing import Optional, Callable\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from einops import rearrange, repeat\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "try:\n",
        "\n",
        "    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# an alternative for mamba_ssm (in which causal_conv1d is needed)\n",
        "try:\n",
        "    from selective_scan import selective_scan_fn as selective_scan_fn_v1\n",
        "    from selective_scan import selective_scan_ref as selective_scan_ref_v1\n",
        "except:\n",
        "    pass\n",
        "\n",
        "DropPath.__repr__ = lambda self: f\"timm.DropPath({self.drop_prob})\"\n",
        "\n",
        "\n",
        "def flops_selective_scan_ref(B=1, L=256, D=768, N=16, with_D=True, with_Z=False, with_Group=True, with_complex=False):\n",
        "    \"\"\"\n",
        "    u: r(B D L)\n",
        "    delta: r(B D L)\n",
        "    A: r(D N)\n",
        "    B: r(B N L)\n",
        "    C: r(B N L)\n",
        "    D: r(D)\n",
        "    z: r(B D L)\n",
        "    delta_bias: r(D), fp32\n",
        "\n",
        "    ignores:\n",
        "        [.float(), +, .softplus, .shape, new_zeros, repeat, stack, to(dtype), silu]\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    # fvcore.nn.jit_handles\n",
        "    def get_flops_einsum(input_shapes, equation):\n",
        "        np_arrs = [np.zeros(s) for s in input_shapes]\n",
        "        optim = np.einsum_path(equation, *np_arrs, optimize=\"optimal\")[1]\n",
        "        for line in optim.split(\"\\n\"):\n",
        "            if \"optimized flop\" in line.lower():\n",
        "                # divided by 2 because we count MAC (multiply-add counted as one flop)\n",
        "                flop = float(np.floor(float(line.split(\":\")[-1]) / 2))\n",
        "                return flop\n",
        "\n",
        "\n",
        "    assert not with_complex\n",
        "\n",
        "    flops = 0 # below code flops = 0\n",
        "    if False:\n",
        "        ...\n",
        "        \"\"\"\n",
        "        dtype_in = u.dtype\n",
        "        u = u.float()\n",
        "        delta = delta.float()\n",
        "        if delta_bias is not None:\n",
        "            delta = delta + delta_bias[..., None].float()\n",
        "        if delta_softplus:\n",
        "            delta = F.softplus(delta)\n",
        "        batch, dim, dstate = u.shape[0], A.shape[0], A.shape[1]\n",
        "        is_variable_B = B.dim() >= 3\n",
        "        is_variable_C = C.dim() >= 3\n",
        "        if A.is_complex():\n",
        "            if is_variable_B:\n",
        "                B = torch.view_as_complex(rearrange(B.float(), \"... (L two) -> ... L two\", two=2))\n",
        "            if is_variable_C:\n",
        "                C = torch.view_as_complex(rearrange(C.float(), \"... (L two) -> ... L two\", two=2))\n",
        "        else:\n",
        "            B = B.float()\n",
        "            C = C.float()\n",
        "        x = A.new_zeros((batch, dim, dstate))\n",
        "        ys = []\n",
        "        \"\"\"\n",
        "\n",
        "    flops += get_flops_einsum([[B, D, L], [D, N]], \"bdl,dn->bdln\")\n",
        "    if with_Group:\n",
        "        flops += get_flops_einsum([[B, D, L], [B, N, L], [B, D, L]], \"bdl,bnl,bdl->bdln\")\n",
        "    else:\n",
        "        flops += get_flops_einsum([[B, D, L], [B, D, N, L], [B, D, L]], \"bdl,bdnl,bdl->bdln\")\n",
        "    if False:\n",
        "        ...\n",
        "        \"\"\"\n",
        "        deltaA = torch.exp(torch.einsum('bdl,dn->bdln', delta, A))\n",
        "        if not is_variable_B:\n",
        "            deltaB_u = torch.einsum('bdl,dn,bdl->bdln', delta, B, u)\n",
        "        else:\n",
        "            if B.dim() == 3:\n",
        "                deltaB_u = torch.einsum('bdl,bnl,bdl->bdln', delta, B, u)\n",
        "            else:\n",
        "                B = repeat(B, \"B G N L -> B (G H) N L\", H=dim // B.shape[1])\n",
        "                deltaB_u = torch.einsum('bdl,bdnl,bdl->bdln', delta, B, u)\n",
        "        if is_variable_C and C.dim() == 4:\n",
        "            C = repeat(C, \"B G N L -> B (G H) N L\", H=dim // C.shape[1])\n",
        "        last_state = None\n",
        "        \"\"\"\n",
        "\n",
        "    in_for_flops = B * D * N\n",
        "    if with_Group:\n",
        "        in_for_flops += get_flops_einsum([[B, D, N], [B, D, N]], \"bdn,bdn->bd\")\n",
        "    else:\n",
        "        in_for_flops += get_flops_einsum([[B, D, N], [B, N]], \"bdn,bn->bd\")\n",
        "    flops += L * in_for_flops\n",
        "    if False:\n",
        "        ...\n",
        "        \"\"\"\n",
        "        for i in range(u.shape[2]):\n",
        "            x = deltaA[:, :, i] * x + deltaB_u[:, :, i]\n",
        "            if not is_variable_C:\n",
        "                y = torch.einsum('bdn,dn->bd', x, C)\n",
        "            else:\n",
        "                if C.dim() == 3:\n",
        "                    y = torch.einsum('bdn,bn->bd', x, C[:, :, i])\n",
        "                else:\n",
        "                    y = torch.einsum('bdn,bdn->bd', x, C[:, :, :, i])\n",
        "            if i == u.shape[2] - 1:\n",
        "                last_state = x\n",
        "            if y.is_complex():\n",
        "                y = y.real * 2\n",
        "            ys.append(y)\n",
        "        y = torch.stack(ys, dim=2) # (batch dim L)\n",
        "        \"\"\"\n",
        "\n",
        "    if with_D:\n",
        "        flops += B * D * L\n",
        "    if with_Z:\n",
        "        flops += B * D * L\n",
        "    if False:\n",
        "        ...\n",
        "        \"\"\"\n",
        "        out = y if D is None else y + u * rearrange(D, \"d -> d 1\")\n",
        "        if z is not None:\n",
        "            out = out * F.silu(z)\n",
        "        out = out.to(dtype=dtype_in)\n",
        "        \"\"\"\n",
        "\n",
        "    return flops\n",
        "\n",
        "\n",
        "class PatchEmbed2D(nn.Module):\n",
        "    r\"\"\" Image to Patch Embedding\n",
        "    Args:\n",
        "        patch_size (int): Patch token size. Default: 4.\n",
        "        in_chans (int): Number of input image channels. Default: 3.\n",
        "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
        "    \"\"\"\n",
        "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None, **kwargs):\n",
        "        super().__init__()\n",
        "        if isinstance(patch_size, int):\n",
        "            patch_size = (patch_size, patch_size)\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        if norm_layer is not None:\n",
        "            self.norm = norm_layer(embed_dim)\n",
        "        else:\n",
        "            self.norm = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x).permute(0, 2, 3, 1)\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchMerging2D(nn.Module):\n",
        "    r\"\"\" Patch Merging Layer.\n",
        "    Args:\n",
        "        input_resolution (tuple[int]): Resolution of input feature.\n",
        "        dim (int): Number of input channels.\n",
        "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
        "        self.norm = norm_layer(4 * dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, H, W, C = x.shape\n",
        "\n",
        "        SHAPE_FIX = [-1, -1]\n",
        "        if (W % 2 != 0) or (H % 2 != 0):\n",
        "            print(f\"Warning, x.shape {x.shape} is not match even ===========\", flush=True)\n",
        "            SHAPE_FIX[0] = H // 2\n",
        "            SHAPE_FIX[1] = W // 2\n",
        "\n",
        "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
        "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
        "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
        "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
        "\n",
        "        if SHAPE_FIX[0] > 0:\n",
        "            x0 = x0[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
        "            x1 = x1[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
        "            x2 = x2[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
        "            x3 = x3[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
        "\n",
        "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
        "        x = x.view(B, H//2, W//2, 4 * C)  # B H/2*W/2 4*C\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchExpand2D(nn.Module):\n",
        "    def __init__(self, dim, dim_scale=2, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.dim = dim*2\n",
        "        self.dim_scale = dim_scale\n",
        "        self.expand = nn.Linear(self.dim, dim_scale*self.dim, bias=False)\n",
        "        self.norm = norm_layer(self.dim // dim_scale)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, H, W, C = x.shape\n",
        "        x = self.expand(x)\n",
        "\n",
        "        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=self.dim_scale, p2=self.dim_scale, c=C//self.dim_scale)\n",
        "        x= self.norm(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Final_PatchExpand2D(nn.Module):\n",
        "    def __init__(self, dim, dim_scale=4, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.dim_scale = dim_scale\n",
        "        self.expand = nn.Linear(self.dim, dim_scale*self.dim, bias=False)\n",
        "        self.norm = norm_layer(self.dim // dim_scale)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, H, W, C = x.shape\n",
        "        x = self.expand(x)\n",
        "\n",
        "        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=self.dim_scale, p2=self.dim_scale, c=C//self.dim_scale)\n",
        "        x= self.norm(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class SS2D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model,\n",
        "        d_state=16,\n",
        "        # d_state=\"auto\", # 20240109\n",
        "        d_conv=3,\n",
        "        expand=2,\n",
        "        dt_rank=\"auto\",\n",
        "        dt_min=0.001,\n",
        "        dt_max=0.1,\n",
        "        dt_init=\"random\",\n",
        "        dt_scale=1.0,\n",
        "        dt_init_floor=1e-4,\n",
        "        dropout=0.,\n",
        "        conv_bias=True,\n",
        "        bias=False,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "        # self.d_state = math.ceil(self.d_model / 6) if d_state == \"auto\" else d_model # 20240109\n",
        "        self.d_conv = d_conv\n",
        "        self.expand = expand\n",
        "        self.d_inner = int(self.expand * self.d_model)\n",
        "        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
        "\n",
        "        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)\n",
        "        self.conv2d = nn.Conv2d(\n",
        "            in_channels=self.d_inner,\n",
        "            out_channels=self.d_inner,\n",
        "            groups=self.d_inner,\n",
        "            bias=conv_bias,\n",
        "            kernel_size=d_conv,\n",
        "            padding=(d_conv - 1) // 2,\n",
        "            **factory_kwargs,\n",
        "        )\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        self.x_proj = (\n",
        "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
        "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
        "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
        "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
        "        )\n",
        "        self.x_proj_weight = nn.Parameter(torch.stack([t.weight for t in self.x_proj], dim=0)) # (K=4, N, inner)\n",
        "        del self.x_proj\n",
        "\n",
        "        self.dt_projs = (\n",
        "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
        "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
        "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
        "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
        "        )\n",
        "        self.dt_projs_weight = nn.Parameter(torch.stack([t.weight for t in self.dt_projs], dim=0)) # (K=4, inner, rank)\n",
        "        self.dt_projs_bias = nn.Parameter(torch.stack([t.bias for t in self.dt_projs], dim=0)) # (K=4, inner)\n",
        "        del self.dt_projs\n",
        "\n",
        "        self.A_logs = self.A_log_init(self.d_state, self.d_inner, copies=4, merge=True) # (K=4, D, N)\n",
        "        self.Ds = self.D_init(self.d_inner, copies=4, merge=True) # (K=4, D, N)\n",
        "\n",
        "        # self.selective_scan = selective_scan_fn\n",
        "        self.forward_core = self.forward_corev0\n",
        "\n",
        "        self.out_norm = nn.LayerNorm(self.d_inner)\n",
        "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0. else None\n",
        "\n",
        "    @staticmethod\n",
        "    def dt_init(dt_rank, d_inner, dt_scale=1.0, dt_init=\"random\", dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4, **factory_kwargs):\n",
        "        dt_proj = nn.Linear(dt_rank, d_inner, bias=True, **factory_kwargs)\n",
        "\n",
        "        # Initialize special dt projection to preserve variance at initialization\n",
        "        dt_init_std = dt_rank**-0.5 * dt_scale\n",
        "        if dt_init == \"constant\":\n",
        "            nn.init.constant_(dt_proj.weight, dt_init_std)\n",
        "        elif dt_init == \"random\":\n",
        "            nn.init.uniform_(dt_proj.weight, -dt_init_std, dt_init_std)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max\n",
        "        dt = torch.exp(\n",
        "            torch.rand(d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n",
        "            + math.log(dt_min)\n",
        "        ).clamp(min=dt_init_floor)\n",
        "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
        "        with torch.no_grad():\n",
        "            dt_proj.bias.copy_(inv_dt)\n",
        "        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
        "        dt_proj.bias._no_reinit = True\n",
        "\n",
        "        return dt_proj\n",
        "\n",
        "    @staticmethod\n",
        "    def A_log_init(d_state, d_inner, copies=1, device=None, merge=True):\n",
        "        # S4D real initialization\n",
        "        A = repeat(\n",
        "            torch.arange(1, d_state + 1, dtype=torch.float32, device=device),\n",
        "            \"n -> d n\",\n",
        "            d=d_inner,\n",
        "        ).contiguous()\n",
        "        A_log = torch.log(A)  # Keep A_log in fp32\n",
        "        if copies > 1:\n",
        "            A_log = repeat(A_log, \"d n -> r d n\", r=copies)\n",
        "            if merge:\n",
        "                A_log = A_log.flatten(0, 1)\n",
        "        A_log = nn.Parameter(A_log)\n",
        "        A_log._no_weight_decay = True\n",
        "        return A_log\n",
        "\n",
        "    @staticmethod\n",
        "    def D_init(d_inner, copies=1, device=None, merge=True):\n",
        "        # D \"skip\" parameter\n",
        "        D = torch.ones(d_inner, device=device)\n",
        "        if copies > 1:\n",
        "            D = repeat(D, \"n1 -> r n1\", r=copies)\n",
        "            if merge:\n",
        "                D = D.flatten(0, 1)\n",
        "        D = nn.Parameter(D)  # Keep in fp32\n",
        "        D._no_weight_decay = True\n",
        "        return D\n",
        "\n",
        "    def forward_corev0(self, x: torch.Tensor):\n",
        "        self.selective_scan = selective_scan_fn\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        L = H * W\n",
        "        K = 4\n",
        "\n",
        "        x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)], dim=1).view(B, 2, -1, L)\n",
        "        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1) # (b, k, d, l)\n",
        "\n",
        "        x_dbl = torch.einsum(\"b k d l, k c d -> b k c l\", xs.view(B, K, -1, L), self.x_proj_weight)\n",
        "        # x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)\n",
        "        dts, Bs, Cs = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2)\n",
        "        dts = torch.einsum(\"b k r l, k d r -> b k d l\", dts.view(B, K, -1, L), self.dt_projs_weight)\n",
        "        # dts = dts + self.dt_projs_bias.view(1, K, -1, 1)\n",
        "\n",
        "        xs = xs.float().view(B, -1, L) # (b, k * d, l)\n",
        "        dts = dts.contiguous().float().view(B, -1, L) # (b, k * d, l)\n",
        "        Bs = Bs.float().view(B, K, -1, L) # (b, k, d_state, l)\n",
        "        Cs = Cs.float().view(B, K, -1, L) # (b, k, d_state, l)\n",
        "        Ds = self.Ds.float().view(-1) # (k * d)\n",
        "        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)\n",
        "        dt_projs_bias = self.dt_projs_bias.float().view(-1) # (k * d)\n",
        "\n",
        "        out_y = self.selective_scan(\n",
        "            xs, dts,\n",
        "            As, Bs, Cs, Ds, z=None,\n",
        "            delta_bias=dt_projs_bias,\n",
        "            delta_softplus=True,\n",
        "            return_last_state=False,\n",
        "        ).view(B, K, -1, L)\n",
        "        assert out_y.dtype == torch.float\n",
        "\n",
        "        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)\n",
        "        wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
        "        invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
        "\n",
        "        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y\n",
        "\n",
        "    # an alternative to forward_corev1\n",
        "    def forward_corev1(self, x: torch.Tensor):\n",
        "        self.selective_scan = selective_scan_fn_v1\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        L = H * W\n",
        "        K = 4\n",
        "\n",
        "        x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)], dim=1).view(B, 2, -1, L)\n",
        "        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1) # (b, k, d, l)\n",
        "\n",
        "        x_dbl = torch.einsum(\"b k d l, k c d -> b k c l\", xs.view(B, K, -1, L), self.x_proj_weight)\n",
        "        # x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)\n",
        "        dts, Bs, Cs = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2)\n",
        "        dts = torch.einsum(\"b k r l, k d r -> b k d l\", dts.view(B, K, -1, L), self.dt_projs_weight)\n",
        "        # dts = dts + self.dt_projs_bias.view(1, K, -1, 1)\n",
        "\n",
        "        xs = xs.float().view(B, -1, L) # (b, k * d, l)\n",
        "        dts = dts.contiguous().float().view(B, -1, L) # (b, k * d, l)\n",
        "        Bs = Bs.float().view(B, K, -1, L) # (b, k, d_state, l)\n",
        "        Cs = Cs.float().view(B, K, -1, L) # (b, k, d_state, l)\n",
        "        Ds = self.Ds.float().view(-1) # (k * d)\n",
        "        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)\n",
        "        dt_projs_bias = self.dt_projs_bias.float().view(-1) # (k * d)\n",
        "\n",
        "        out_y = self.selective_scan(\n",
        "            xs, dts,\n",
        "            As, Bs, Cs, Ds,\n",
        "            delta_bias=dt_projs_bias,\n",
        "            delta_softplus=True,\n",
        "        ).view(B, K, -1, L)\n",
        "        assert out_y.dtype == torch.float\n",
        "\n",
        "        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)\n",
        "        wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
        "        invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
        "\n",
        "        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y\n",
        "\n",
        "    def forward(self, x: torch.Tensor, **kwargs):\n",
        "        B, H, W, C = x.shape\n",
        "\n",
        "        xz = self.in_proj(x)\n",
        "        x, z = xz.chunk(2, dim=-1) # (b, h, w, d)\n",
        "\n",
        "        x = x.permute(0, 3, 1, 2).contiguous()\n",
        "        x = self.act(self.conv2d(x)) # (b, d, h, w)\n",
        "        y1, y2, y3, y4 = self.forward_core(x)\n",
        "        assert y1.dtype == torch.float32\n",
        "        y = y1 + y2 + y3 + y4\n",
        "        y = torch.transpose(y, dim0=1, dim1=2).contiguous().view(B, H, W, -1)\n",
        "        y = self.out_norm(y)\n",
        "        y = y * F.silu(z)\n",
        "        out = self.out_proj(y)\n",
        "        if self.dropout is not None:\n",
        "            out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class VSSBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_dim: int = 0,\n",
        "        drop_path: float = 0,\n",
        "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
        "        attn_drop_rate: float = 0,\n",
        "        d_state: int = 16,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ln_1 = norm_layer(hidden_dim)\n",
        "        self.self_attention = SS2D(d_model=hidden_dim, dropout=attn_drop_rate, d_state=d_state, **kwargs)\n",
        "        self.drop_path = DropPath(drop_path)\n",
        "\n",
        "    def forward(self, input: torch.Tensor):\n",
        "        x = input + self.drop_path(self.self_attention(self.ln_1(input)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class VSSLayer(nn.Module):\n",
        "    \"\"\" A basic Swin Transformer layer for one stage.\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        depth (int): Number of blocks.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
        "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
        "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        depth,\n",
        "        attn_drop=0.,\n",
        "        drop_path=0.,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        downsample=None,\n",
        "        use_checkpoint=False,\n",
        "        d_state=16,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            VSSBlock(\n",
        "                hidden_dim=dim,\n",
        "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                norm_layer=norm_layer,\n",
        "                attn_drop_rate=attn_drop,\n",
        "                d_state=d_state,\n",
        "            )\n",
        "            for i in range(depth)])\n",
        "\n",
        "        if True: # is this really applied? Yes, but been overriden later in VSSM!\n",
        "            def _init_weights(module: nn.Module):\n",
        "                for name, p in module.named_parameters():\n",
        "                    if name in [\"out_proj.weight\"]:\n",
        "                        p = p.clone().detach_() # fake init, just to keep the seed ....\n",
        "                        nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
        "            self.apply(_init_weights)\n",
        "\n",
        "        if downsample is not None:\n",
        "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for blk in self.blocks:\n",
        "            if self.use_checkpoint:\n",
        "                x = checkpoint.checkpoint(blk, x)\n",
        "            else:\n",
        "                x = blk(x)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class VSSLayer_up(nn.Module):\n",
        "    \"\"\" A basic Swin Transformer layer for one stage.\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        depth (int): Number of blocks.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
        "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
        "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        depth,\n",
        "        attn_drop=0.,\n",
        "        drop_path=0.,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        upsample=None,\n",
        "        use_checkpoint=False,\n",
        "        d_state=16,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            VSSBlock(\n",
        "                hidden_dim=dim,\n",
        "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                norm_layer=norm_layer,\n",
        "                attn_drop_rate=attn_drop,\n",
        "                d_state=d_state,\n",
        "            )\n",
        "            for i in range(depth)])\n",
        "\n",
        "        if True: # is this really applied? Yes, but been overriden later in VSSM!\n",
        "            def _init_weights(module: nn.Module):\n",
        "                for name, p in module.named_parameters():\n",
        "                    if name in [\"out_proj.weight\"]:\n",
        "                        p = p.clone().detach_() # fake init, just to keep the seed ....\n",
        "                        nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
        "            self.apply(_init_weights)\n",
        "\n",
        "        if upsample is not None:\n",
        "            self.upsample = upsample(dim=dim, norm_layer=norm_layer)\n",
        "        else:\n",
        "            self.upsample = None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.upsample is not None:\n",
        "            x = self.upsample(x)\n",
        "        for blk in self.blocks:\n",
        "            if self.use_checkpoint:\n",
        "                x = checkpoint.checkpoint(blk, x)\n",
        "            else:\n",
        "                x = blk(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class VSSM(nn.Module):\n",
        "    def __init__(self, patch_size=4, in_chans=3, num_classes=1000, depths=[2, 2, 9, 2], depths_decoder=[2, 9, 2, 2],\n",
        "                 dims=[96, 192, 384, 768], dims_decoder=[768, 384, 192, 96], d_state=16, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
        "                 norm_layer=nn.LayerNorm, patch_norm=True,\n",
        "                 use_checkpoint=False, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = len(depths)\n",
        "        if isinstance(dims, int):\n",
        "            dims = [int(dims * 2 ** i_layer) for i_layer in range(self.num_layers)]\n",
        "        self.embed_dim = dims[0]\n",
        "        self.num_features = dims[-1]\n",
        "        self.dims = dims\n",
        "\n",
        "        self.patch_embed = PatchEmbed2D(patch_size=patch_size, in_chans=in_chans, embed_dim=self.embed_dim,\n",
        "            norm_layer=norm_layer if patch_norm else None)\n",
        "\n",
        "        # WASTED absolute position embedding ======================\n",
        "        self.ape = False\n",
        "        # self.ape = False\n",
        "        # drop_rate = 0.0\n",
        "        if self.ape:\n",
        "            self.patches_resolution = self.patch_embed.patches_resolution\n",
        "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, *self.patches_resolution, self.embed_dim))\n",
        "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
        "        dpr_decoder = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths_decoder))][::-1]\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i_layer in range(self.num_layers):\n",
        "            layer = VSSLayer(\n",
        "                dim=dims[i_layer],\n",
        "                depth=depths[i_layer],\n",
        "                d_state=math.ceil(dims[0] / 6) if d_state is None else d_state, # 20240109\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
        "                norm_layer=norm_layer,\n",
        "                downsample=PatchMerging2D if (i_layer < self.num_layers - 1) else None,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "            )\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        self.layers_up = nn.ModuleList()\n",
        "        for i_layer in range(self.num_layers):\n",
        "            layer = VSSLayer_up(\n",
        "                dim=dims_decoder[i_layer],\n",
        "                depth=depths_decoder[i_layer],\n",
        "                d_state=math.ceil(dims[0] / 6) if d_state is None else d_state, # 20240109\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr_decoder[sum(depths_decoder[:i_layer]):sum(depths_decoder[:i_layer + 1])],\n",
        "                norm_layer=norm_layer,\n",
        "                upsample=PatchExpand2D if (i_layer != 0) else None,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "            )\n",
        "            self.layers_up.append(layer)\n",
        "\n",
        "        self.final_up = Final_PatchExpand2D(dim=dims_decoder[-1], dim_scale=4, norm_layer=norm_layer)\n",
        "        self.final_conv = nn.Conv2d(dims_decoder[-1]//4, num_classes, 1)\n",
        "\n",
        "        # self.norm = norm_layer(self.num_features)\n",
        "        # self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
        "        # self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m: nn.Module):\n",
        "        \"\"\"\n",
        "        out_proj.weight which is previously initilized in VSSBlock, would be cleared in nn.Linear\n",
        "        no fc.weight found in the any of the model parameters\n",
        "        no nn.Embedding found in the any of the model parameters\n",
        "        so the thing is, VSSBlock initialization is useless\n",
        "\n",
        "        Conv2D is not intialized !!!\n",
        "        \"\"\"\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'absolute_pos_embed'}\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        return {'relative_position_bias_table'}\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        skip_list = []\n",
        "        x = self.patch_embed(x)\n",
        "        if self.ape:\n",
        "            x = x + self.absolute_pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            skip_list.append(x)\n",
        "            x = layer(x)\n",
        "        return x, skip_list\n",
        "\n",
        "    def forward_features_up(self, x, skip_list):\n",
        "        for inx, layer_up in enumerate(self.layers_up):\n",
        "            if inx == 0:\n",
        "                x = layer_up(x)\n",
        "            else:\n",
        "                x = layer_up(x+skip_list[-inx])\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward_final(self, x):\n",
        "        x = self.final_up(x)\n",
        "        x = x.permute(0,3,1,2)\n",
        "        x = self.final_conv(x)\n",
        "        return x\n",
        "\n",
        "    def forward_backbone(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        if self.ape:\n",
        "            x = x + self.absolute_pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, skip_list = self.forward_features(x)\n",
        "        x = self.forward_features_up(x, skip_list)\n",
        "        x = self.forward_final(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxvhxdSN5zsY"
      },
      "source": [
        "## Check the details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GLF68rwO5zsY"
      },
      "outputs": [],
      "source": [
        "my_model = VSSBlock(hidden_dim = 64).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ethyb2Dm5zsZ",
        "outputId": "2ff9e803-fb37-4cea-fa04-3ae1732a8672"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VSSBlock(\n",
              "  (ln_1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
              "  (self_attention): SS2D(\n",
              "    (in_proj): Linear(in_features=64, out_features=256, bias=False)\n",
              "    (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "    (act): SiLU()\n",
              "    (out_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    (out_proj): Linear(in_features=128, out_features=64, bias=False)\n",
              "  )\n",
              "  (drop_path): timm.DropPath(0)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "my_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nNi49xP5zsZ",
        "outputId": "724f0fd8-183c-47fe-a050-0918e3be4c21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 128, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "x = torch.randn(1,128,128,64).cuda()\n",
        "output = my_model(x)\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM06MSx95zsZ"
      },
      "source": [
        "# Make the Models (V1 & V2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Hamk5uy35zsZ"
      },
      "outputs": [],
      "source": [
        "def calculate_params_in_millions(model):\n",
        "  \"\"\"Calculates the number of parameters in a PyTorch model in millions.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model.\n",
        "\n",
        "  Returns:\n",
        "    The number of parameters in millions.\n",
        "  \"\"\"\n",
        "  num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  return num_params / 1e6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JI1yrVpt5zsZ"
      },
      "outputs": [],
      "source": [
        "class VSSBlockCustom(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_dim: int = 0,\n",
        "        drop_path: float = 0,\n",
        "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
        "        attn_drop_rate: float = 0,\n",
        "        d_state: int = 16,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ln_1 = norm_layer(hidden_dim)\n",
        "        self.self_attention = SS2D(d_model=hidden_dim, dropout=attn_drop_rate, d_state=d_state, **kwargs)\n",
        "        self.drop_path = DropPath(drop_path)\n",
        "\n",
        "    def forward(self, input: torch.Tensor):\n",
        "        x = input + self.drop_path(self.self_attention(self.ln_1(input)))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-daFbsoK5zsZ"
      },
      "source": [
        "## V1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Y7tnkjJ65zsZ"
      },
      "outputs": [],
      "source": [
        "class V1(nn.Module):\n",
        "    def __init__(\n",
        "    self,\n",
        "    hidden_dim: int = 0,\n",
        "    drop_path: float = 0,\n",
        "    norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
        "    attn_drop_rate: float = 0,\n",
        "    d_state: int = 16,\n",
        "    init_value: float =1.0,\n",
        "    **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        hidden_dim = hidden_dim\n",
        "        drop_path = drop_path\n",
        "        norm_layer = norm_layer\n",
        "        attn_drop_rate = attn_drop_rate\n",
        "\n",
        "        self.vssm = VSSBlockCustom(hidden_dim = hidden_dim,\n",
        "                                   drop_path = drop_path,\n",
        "                                   norm_layer = norm_layer,\n",
        "                                   attn_drop_rate = attn_drop_rate)\n",
        "\n",
        "        self.lambda_ =  nn.Parameter(init_value * torch.ones(1, 1, hidden_dim),\n",
        "                                    requires_grad = True) # B H W C\n",
        "\n",
        "\n",
        "        self.conv_bbone = nn.Sequential(nn.Conv2d(in_channels = hidden_dim,\n",
        "                                                  out_channels = hidden_dim,\n",
        "                                                  kernel_size = 3,\n",
        "                                                  stride = 1,\n",
        "                                                  padding = 1,\n",
        "                                                  groups = hidden_dim),\n",
        "                                        nn.Conv2d(in_channels= hidden_dim,\n",
        "                                                  out_channels = hidden_dim,\n",
        "                                                  kernel_size = 1),\n",
        "                                        nn.SiLU(),\n",
        "                                        nn.BatchNorm2d(num_features=hidden_dim)\n",
        "                                       )\n",
        "\n",
        "        self.beta =  nn.Parameter(init_value * torch.ones(hidden_dim, 1, 1),\n",
        "                                    requires_grad = True)\n",
        "\n",
        "        self.mlp = nn.Sequential(nn.Conv2d(in_channels= hidden_dim,\n",
        "                                           out_channels = hidden_dim,\n",
        "                                           kernel_size = 1),\n",
        "                                 nn.SiLU(),\n",
        "                                 nn.BatchNorm2d(num_features=hidden_dim))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        b, h, w, c = x.shape # Must be B, H, W, C\n",
        "\n",
        "        x_ = x.clone() # first residual initialization\n",
        "\n",
        "        y = x + self.vssm(x) * self.lambda_ # first Residual  + mamba\n",
        "\n",
        "        ####\n",
        "        # CNN\n",
        "        y = y.view(b , c, h, w).contiguous() # Conv requires B, C, H, W\n",
        "        y_cnn = self.conv_bbone(y)\n",
        "\n",
        "        ###\n",
        "        # first reidual + param (for CNN)\n",
        "        x_2 = self.beta * x.view(b , c, h, w).contiguous() # second residual parametrization\n",
        "\n",
        "\n",
        "        y_2 = y_cnn + x_2  # first Residual  + cnn\n",
        "\n",
        "        y_2 = self.mlp(y_2)\n",
        "\n",
        "        y_2 = y_2.view(b, h, w, c).contiguous()\n",
        "\n",
        "\n",
        "        return y_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gBaec-9y5zsZ"
      },
      "outputs": [],
      "source": [
        "model = V1(hidden_dim = 64).cuda()\n",
        "x = torch.randn(1,128,128,64).cuda()\n",
        "output = model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coNK_l9l5zsZ",
        "outputId": "b55dc87e-69dc-4033-deae-70e1d51c2d97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.06528, torch.Size([1, 128, 128, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "calculate_params_in_millions(model), output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "f1PNrj2a5zsa"
      },
      "outputs": [],
      "source": [
        "class VSSLayer_V1(nn.Module):\n",
        "    \"\"\" A basic Swin Transformer layer for one stage.\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        depth (int): Number of blocks.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
        "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
        "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        depth,\n",
        "        attn_drop=0.,\n",
        "        drop_path=0.,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        downsample=None,\n",
        "        use_checkpoint=False,\n",
        "        d_state=16,\n",
        "        init_value: float =1.0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            VSSBlock(\n",
        "                hidden_dim=dim,\n",
        "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                norm_layer=norm_layer,\n",
        "                attn_drop_rate=attn_drop,\n",
        "                d_state=d_state,\n",
        "            )\n",
        "            for i in range(depth)])\n",
        "\n",
        "#         self.blocks = nn.ModuleList([\n",
        "#             V1(\n",
        "#                 hidden_dim = dim,\n",
        "#                 drop_path = drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "#                 norm_layer = norm_layer,\n",
        "#                 attn_drop_rate = attn_drop,\n",
        "#                 d_state = d_state,\n",
        "#                 init_value = init_value\n",
        "#             )\n",
        "#             for i in range(depth)\n",
        "#         ])\n",
        "\n",
        "        if True: # is this really applied? Yes, but been overriden later in VSSM!\n",
        "            def _init_weights(module: nn.Module):\n",
        "                for name, p in module.named_parameters():\n",
        "                    if name in [\"out_proj.weight\"]:\n",
        "                        p = p.clone().detach_() # fake init, just to keep the seed ....\n",
        "                        nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
        "            self.apply(_init_weights)\n",
        "\n",
        "        if downsample is not None:\n",
        "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for blk in self.blocks:\n",
        "            if self.use_checkpoint:\n",
        "                x = checkpoint.checkpoint(blk, x)\n",
        "            else:\n",
        "                x = blk(x)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class VSSLayer_up_V1(nn.Module):\n",
        "    \"\"\" A basic Swin Transformer layer for one stage.\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        depth (int): Number of blocks.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
        "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
        "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        depth,\n",
        "        attn_drop=0.,\n",
        "        drop_path=0.,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        upsample=None,\n",
        "        use_checkpoint=False,\n",
        "        d_state=16,\n",
        "        init_value: float =1.0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "\n",
        "#         self.blocks = nn.ModuleList([\n",
        "#             VSSBlock(\n",
        "#                 hidden_dim=dim,\n",
        "#                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "#                 norm_layer=norm_layer,\n",
        "#                 attn_drop_rate=attn_drop,\n",
        "#                 d_state=d_state,\n",
        "#             )\n",
        "#             for i in range(depth)])\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            V1(\n",
        "                hidden_dim = dim,\n",
        "                drop_path = drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                norm_layer = norm_layer,\n",
        "                attn_drop_rate = attn_drop,\n",
        "                d_state = d_state,\n",
        "                init_value = init_value\n",
        "            )\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "\n",
        "        if True: # is this really applied? Yes, but been overriden later in VSSM!\n",
        "            def _init_weights(module: nn.Module):\n",
        "                for name, p in module.named_parameters():\n",
        "                    if name in [\"out_proj.weight\"]:\n",
        "                        p = p.clone().detach_() # fake init, just to keep the seed ....\n",
        "                        nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
        "            self.apply(_init_weights)\n",
        "\n",
        "        if upsample is not None:\n",
        "            self.upsample = upsample(dim=dim, norm_layer=norm_layer)\n",
        "        else:\n",
        "            self.upsample = None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.upsample is not None:\n",
        "            x = self.upsample(x)\n",
        "        for blk in self.blocks:\n",
        "            if self.use_checkpoint:\n",
        "                x = checkpoint.checkpoint(blk, x)\n",
        "            else:\n",
        "                x = blk(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RilfPPNv5zsa",
        "outputId": "24fd224d-4e9c-486c-a5c5-c7f45547f2e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 128, 128, 64]), torch.Size([1, 128, 128, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "layer = VSSLayer(dim = 64, depth = 2).cuda()\n",
        "layer_up = VSSLayer_up_V1(dim = 64, depth = 2).cuda()\n",
        "\n",
        "a = torch.randn(1,128,128,64).cuda()\n",
        "\n",
        "layer(a).shape, layer_up(x).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t07eWiLd5zsa",
        "outputId": "8088bc22-009e-4d16-b7c8-30133917130f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VSSLayer(\n",
              "  (blocks): ModuleList(\n",
              "    (0-1): 2 x VSSBlock(\n",
              "      (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (self_attention): SS2D(\n",
              "        (in_proj): Linear(in_features=64, out_features=256, bias=False)\n",
              "        (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "        (act): SiLU()\n",
              "        (out_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (out_proj): Linear(in_features=128, out_features=64, bias=False)\n",
              "      )\n",
              "      (drop_path): timm.DropPath(0.0)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTvWJ1mr5zsa",
        "outputId": "8f9a0acd-a24d-46e0-95f8-dc0b90921ecf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VSSLayer_up_V1(\n",
              "  (blocks): ModuleList(\n",
              "    (0-1): 2 x V1(\n",
              "      (vssm): VSSBlockCustom(\n",
              "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): SS2D(\n",
              "          (in_proj): Linear(in_features=64, out_features=256, bias=False)\n",
              "          (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "          (act): SiLU()\n",
              "          (out_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (out_proj): Linear(in_features=128, out_features=64, bias=False)\n",
              "        )\n",
              "        (drop_path): timm.DropPath(0.0)\n",
              "      )\n",
              "      (conv_bbone): Sequential(\n",
              "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
              "        (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): SiLU()\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (mlp): Sequential(\n",
              "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): SiLU()\n",
              "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "layer_up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OajVbqTN5zsa"
      },
      "source": [
        "## V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "WzkJbf3s5zsa"
      },
      "outputs": [],
      "source": [
        "class V2(nn.Module):\n",
        "    def __init__(\n",
        "    self,\n",
        "    hidden_dim: int = 0,\n",
        "    drop_path: float = 0,\n",
        "    norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
        "    attn_drop_rate: float = 0,\n",
        "    d_state: int = 16,\n",
        "    init_value: float =1.0,\n",
        "    **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        hidden_dim = hidden_dim\n",
        "        drop_path = drop_path\n",
        "        norm_layer = norm_layer\n",
        "        attn_drop_rate = attn_drop_rate\n",
        "\n",
        "\n",
        "        self.vssm = VSSBlockCustom(hidden_dim = hidden_dim,\n",
        "                                   drop_path = drop_path,\n",
        "                                   norm_layer = norm_layer,\n",
        "                                   attn_drop_rate = attn_drop_rate)\n",
        "\n",
        "        self.lambda_ =  nn.Parameter(init_value * torch.ones(1, 1, hidden_dim),\n",
        "                                    requires_grad = True) # 1,1,1, C\n",
        "\n",
        "\n",
        "        self.conv_bbone = nn.Sequential(nn.Conv2d(in_channels = hidden_dim,\n",
        "                                                  out_channels = hidden_dim,\n",
        "                                                  kernel_size = 3,\n",
        "                                                  stride = 1,\n",
        "                                                  padding = 1,\n",
        "                                                  groups = hidden_dim),\n",
        "                                        nn.BatchNorm2d(num_features=hidden_dim),\n",
        "                                        nn.GELU(),\n",
        "                                        nn.Conv2d(in_channels= hidden_dim,\n",
        "                                                  out_channels = hidden_dim,\n",
        "                                                  kernel_size = 1)\n",
        "                                       )\n",
        "\n",
        "        self.beta = self.beta_ =  nn.Parameter(init_value * torch.ones(hidden_dim, 1, 1),\n",
        "                                    requires_grad = True) # 1 C 1 1\n",
        "\n",
        "        self.mlp = nn.Sequential(nn.Conv2d(in_channels= hidden_dim,\n",
        "                                           out_channels = hidden_dim,\n",
        "                                           kernel_size = 1),\n",
        "                                 nn.SiLU(),\n",
        "                                 nn.BatchNorm2d(num_features=hidden_dim))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        b, h, w, c = x.shape # Must be B, H, W, C\n",
        "\n",
        "        x_ = x.clone() # first residual initialization\n",
        "\n",
        "        ## Mamba\n",
        "\n",
        "        x = self.vssm(x)\n",
        "\n",
        "        x = x * self.lambda_ ## Parametrization\n",
        "\n",
        "        ####  Second Res connection ####\n",
        "        x_2 = x.clone() # Specialized for second residual block\n",
        "\n",
        "        ####       B H W C          ####\n",
        "\n",
        "        x = x + x_ # First Residual block\n",
        "\n",
        "        ## CNN\n",
        "\n",
        "        x = x.view(b , c, h, w).contiguous() # Conv requires B, C, H, W\n",
        "\n",
        "        x_cnn = self.conv_bbone(x)\n",
        "\n",
        "        x_cnn = x_cnn * self.beta\n",
        "\n",
        "        ## Second Res Connection usage\n",
        "\n",
        "        y = x_2.view(b , c, h, w).contiguous() + x_cnn\n",
        "\n",
        "        ## mlp\n",
        "        y = self.mlp(y)\n",
        "\n",
        "        y = y.view(b, h, w, c).contiguous()\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "C6h79vmo5zsa"
      },
      "outputs": [],
      "source": [
        "class VSSLayer_V2(nn.Module):\n",
        "    \"\"\" A basic Swin Transformer layer for one stage.\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        depth (int): Number of blocks.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
        "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
        "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        depth,\n",
        "        attn_drop=0.,\n",
        "        drop_path=0.,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        downsample=None,\n",
        "        use_checkpoint=False,\n",
        "        d_state=16,\n",
        "        init_value: float =1.0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "\n",
        "#         self.blocks = nn.ModuleList([\n",
        "#             VSSBlock(\n",
        "#                 hidden_dim=dim,\n",
        "#                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "#                 norm_layer=norm_layer,\n",
        "#                 attn_drop_rate=attn_drop,\n",
        "#                 d_state=d_state,\n",
        "#             )\n",
        "#             for i in range(depth)])\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            V2(\n",
        "                hidden_dim = dim,\n",
        "                drop_path = drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                norm_layer = norm_layer,\n",
        "                attn_drop_rate = attn_drop,\n",
        "                d_state = d_state,\n",
        "                init_value = init_value\n",
        "            )\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "\n",
        "        if True: # is this really applied? Yes, but been overriden later in VSSM!\n",
        "            def _init_weights(module: nn.Module):\n",
        "                for name, p in module.named_parameters():\n",
        "                    if name in [\"out_proj.weight\"]:\n",
        "                        p = p.clone().detach_() # fake init, just to keep the seed ....\n",
        "                        nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
        "            self.apply(_init_weights)\n",
        "\n",
        "        if downsample is not None:\n",
        "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for blk in self.blocks:\n",
        "            if self.use_checkpoint:\n",
        "                x = checkpoint.checkpoint(blk, x)\n",
        "            else:\n",
        "                x = blk(x)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class VSSLayer_up_V2(nn.Module):\n",
        "    \"\"\" A basic Swin Transformer layer for one stage.\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        depth (int): Number of blocks.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
        "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
        "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        depth,\n",
        "        attn_drop=0.,\n",
        "        drop_path=0.,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        upsample=None,\n",
        "        use_checkpoint=False,\n",
        "        d_state=16,\n",
        "        init_value: float =1.0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "\n",
        "#         self.blocks = nn.ModuleList([\n",
        "#             VSSBlock(\n",
        "#                 hidden_dim=dim,\n",
        "#                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "#                 norm_layer=norm_layer,\n",
        "#                 attn_drop_rate=attn_drop,\n",
        "#                 d_state=d_state,\n",
        "#             )\n",
        "#             for i in range(depth)])\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            V2(\n",
        "                hidden_dim = dim,\n",
        "                drop_path = drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                norm_layer = norm_layer,\n",
        "                attn_drop_rate = attn_drop,\n",
        "                d_state = d_state,\n",
        "                init_value = init_value\n",
        "            )\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "\n",
        "        if True: # is this really applied? Yes, but been overriden later in VSSM!\n",
        "            def _init_weights(module: nn.Module):\n",
        "                for name, p in module.named_parameters():\n",
        "                    if name in [\"out_proj.weight\"]:\n",
        "                        p = p.clone().detach_() # fake init, just to keep the seed ....\n",
        "                        nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
        "            self.apply(_init_weights)\n",
        "\n",
        "        if upsample is not None:\n",
        "            self.upsample = upsample(dim=dim, norm_layer=norm_layer)\n",
        "        else:\n",
        "            self.upsample = None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.upsample is not None:\n",
        "            x = self.upsample(x)\n",
        "        for blk in self.blocks:\n",
        "            if self.use_checkpoint:\n",
        "                x = checkpoint.checkpoint(blk, x)\n",
        "            else:\n",
        "                x = blk(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "753s_tkN5zsa",
        "outputId": "5c7ef0b3-097a-4f4e-80c7-1ea188d6fa4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 128, 128, 64]), 0.06528)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "model2 = V2(hidden_dim = 64).cuda()\n",
        "x = torch.randn(1,128,128,64).cuda()\n",
        "output2 = model2(x)\n",
        "\n",
        "output.shape, calculate_params_in_millions(model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHiplbRc5zsb",
        "outputId": "9d0fdf27-8b5e-4ebd-cc07-2d733d124c89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 128, 128, 64]), torch.Size([1, 128, 128, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "layer = VSSLayer_V2(dim = 64, depth = 2).cuda()\n",
        "layer_up = VSSLayer_up_V2(dim = 64, depth = 2).cuda()\n",
        "\n",
        "a = torch.randn(1,128,128,64).cuda()\n",
        "\n",
        "layer(a).shape, layer_up(x).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCzVaUmU5zsb"
      },
      "source": [
        "## Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-PkuQ9F45zsb"
      },
      "outputs": [],
      "source": [
        "class VSSM_V1(nn.Module):\n",
        "    def __init__(self, patch_size=4, in_chans=3, num_classes=1000, depths=[2, 2, 9, 2], depths_decoder=[2, 9, 2, 2],\n",
        "                 dims=[96, 192, 384, 768], dims_decoder=[768, 384, 192, 96], d_state=16, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
        "                 norm_layer=nn.LayerNorm, patch_norm=True,\n",
        "                 use_checkpoint=False, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = len(depths)\n",
        "        if isinstance(dims, int):\n",
        "            dims = [int(dims * 2 ** i_layer) for i_layer in range(self.num_layers)]\n",
        "        self.embed_dim = dims[0]\n",
        "        self.num_features = dims[-1]\n",
        "        self.dims = dims\n",
        "\n",
        "        self.patch_embed = PatchEmbed2D(patch_size=patch_size, in_chans=in_chans, embed_dim=self.embed_dim,\n",
        "            norm_layer=norm_layer if patch_norm else None)\n",
        "\n",
        "        # WASTED absolute position embedding ======================\n",
        "        self.ape = False\n",
        "        # self.ape = False\n",
        "        # drop_rate = 0.0\n",
        "        if self.ape:\n",
        "            self.patches_resolution = self.patch_embed.patches_resolution\n",
        "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, *self.patches_resolution, self.embed_dim))\n",
        "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
        "        dpr_decoder = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths_decoder))][::-1]\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i_layer in range(self.num_layers):\n",
        "            layer = VSSLayer_V1(\n",
        "                dim=dims[i_layer],\n",
        "                depth=depths[i_layer],\n",
        "                d_state=math.ceil(dims[0] / 6) if d_state is None else d_state, # 20240109\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
        "                norm_layer=norm_layer,\n",
        "                downsample=PatchMerging2D if (i_layer < self.num_layers - 1) else None,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "            )\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        self.layers_up = nn.ModuleList()\n",
        "        for i_layer in range(self.num_layers):\n",
        "            layer = VSSLayer_up_V1(\n",
        "                dim=dims_decoder[i_layer],\n",
        "                depth=depths_decoder[i_layer],\n",
        "                d_state=math.ceil(dims[0] / 6) if d_state is None else d_state, # 20240109\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr_decoder[sum(depths_decoder[:i_layer]):sum(depths_decoder[:i_layer + 1])],\n",
        "                norm_layer=norm_layer,\n",
        "                upsample=PatchExpand2D if (i_layer != 0) else None,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "            )\n",
        "            self.layers_up.append(layer)\n",
        "\n",
        "        self.final_up = Final_PatchExpand2D(dim=dims_decoder[-1], dim_scale=4, norm_layer=norm_layer)\n",
        "        self.final_conv = nn.Conv2d(dims_decoder[-1]//4, num_classes, 1)\n",
        "\n",
        "        # self.norm = norm_layer(self.num_features)\n",
        "        # self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
        "        # self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m: nn.Module):\n",
        "        \"\"\"\n",
        "        out_proj.weight which is previously initilized in VSSBlock, would be cleared in nn.Linear\n",
        "        no fc.weight found in the any of the model parameters\n",
        "        no nn.Embedding found in the any of the model parameters\n",
        "        so the thing is, VSSBlock initialization is useless\n",
        "\n",
        "        Conv2D is not intialized !!!\n",
        "        \"\"\"\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'absolute_pos_embed'}\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        return {'relative_position_bias_table'}\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        skip_list = []\n",
        "        x = self.patch_embed(x)\n",
        "        if self.ape:\n",
        "            x = x + self.absolute_pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            skip_list.append(x)\n",
        "            x = layer(x)\n",
        "        return x, skip_list\n",
        "\n",
        "    def forward_features_up(self, x, skip_list):\n",
        "        for inx, layer_up in enumerate(self.layers_up):\n",
        "            if inx == 0:\n",
        "                x = layer_up(x)\n",
        "            else:\n",
        "                x = layer_up(x+skip_list[-inx])\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward_final(self, x):\n",
        "        x = self.final_up(x)\n",
        "        x = x.permute(0,3,1,2)\n",
        "        x = self.final_conv(x)\n",
        "        return x\n",
        "\n",
        "    def forward_backbone(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        if self.ape:\n",
        "            x = x + self.absolute_pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, skip_list = self.forward_features(x)\n",
        "        x = self.forward_features_up(x, skip_list)\n",
        "        x = self.forward_final(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYB3-aIH5zsb",
        "outputId": "71668058-9bae-4201-8431-1d29a9f2ebb0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1000, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "a = torch.randn(1,3,224,224).cuda()\n",
        "\n",
        "model = VSSM_V1().cuda()\n",
        "\n",
        "model(a).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_a28CzM5zsb",
        "outputId": "3991fca6-a83e-48d1-8c43-ae5e471c86aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49.596856"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "calculate_params_in_millions(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "NXidjjFw5zsb"
      },
      "outputs": [],
      "source": [
        "class VSSM_V2(nn.Module):\n",
        "    def __init__(self, patch_size=4, in_chans=3, num_classes=1000, depths=[2, 2, 9, 2], depths_decoder=[2, 9, 2, 2],\n",
        "                 dims=[96, 192, 384, 768], dims_decoder=[768, 384, 192, 96], d_state=16, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
        "                 norm_layer=nn.LayerNorm, patch_norm=True,\n",
        "                 use_checkpoint=False, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = len(depths)\n",
        "        if isinstance(dims, int):\n",
        "            dims = [int(dims * 2 ** i_layer) for i_layer in range(self.num_layers)]\n",
        "        self.embed_dim = dims[0]\n",
        "        self.num_features = dims[-1]\n",
        "        self.dims = dims\n",
        "\n",
        "        self.patch_embed = PatchEmbed2D(patch_size=patch_size, in_chans=in_chans, embed_dim=self.embed_dim,\n",
        "            norm_layer=norm_layer if patch_norm else None)\n",
        "\n",
        "        # WASTED absolute position embedding ======================\n",
        "        self.ape = False\n",
        "        # self.ape = False\n",
        "        # drop_rate = 0.0\n",
        "        if self.ape:\n",
        "            self.patches_resolution = self.patch_embed.patches_resolution\n",
        "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, *self.patches_resolution, self.embed_dim))\n",
        "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
        "        dpr_decoder = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths_decoder))][::-1]\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i_layer in range(self.num_layers):\n",
        "            layer = VSSLayer(\n",
        "                dim=dims[i_layer],\n",
        "                depth=depths[i_layer],\n",
        "                d_state=math.ceil(dims[0] / 6) if d_state is None else d_state, # 20240109\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
        "                norm_layer=norm_layer,\n",
        "                downsample=PatchMerging2D if (i_layer < self.num_layers - 1) else None,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "            )\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        self.layers_up = nn.ModuleList()\n",
        "        for i_layer in range(self.num_layers):\n",
        "            layer = VSSLayer_up_V2(\n",
        "                dim=dims_decoder[i_layer],\n",
        "                depth=depths_decoder[i_layer],\n",
        "                d_state=math.ceil(dims[0] / 6) if d_state is None else d_state, # 20240109\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr_decoder[sum(depths_decoder[:i_layer]):sum(depths_decoder[:i_layer + 1])],\n",
        "                norm_layer=norm_layer,\n",
        "                upsample=PatchExpand2D if (i_layer != 0) else None,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "            )\n",
        "            self.layers_up.append(layer)\n",
        "\n",
        "        self.final_up = Final_PatchExpand2D(dim=dims_decoder[-1], dim_scale=4, norm_layer=norm_layer)\n",
        "        self.final_conv = nn.Conv2d(dims_decoder[-1]//4, num_classes, 1)\n",
        "\n",
        "        # self.norm = norm_layer(self.num_features)\n",
        "        # self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
        "        # self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m: nn.Module):\n",
        "        \"\"\"\n",
        "        out_proj.weight which is previously initilized in VSSBlock, would be cleared in nn.Linear\n",
        "        no fc.weight found in the any of the model parameters\n",
        "        no nn.Embedding found in the any of the model parameters\n",
        "        so the thing is, VSSBlock initialization is useless\n",
        "\n",
        "        Conv2D is not intialized !!!\n",
        "        \"\"\"\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'absolute_pos_embed'}\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        return {'relative_position_bias_table'}\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        skip_list = []\n",
        "        x = self.patch_embed(x)\n",
        "        if self.ape:\n",
        "            x = x + self.absolute_pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            skip_list.append(x)\n",
        "            x = layer(x)\n",
        "        return x, skip_list\n",
        "\n",
        "    def forward_features_up(self, x, skip_list):\n",
        "        for inx, layer_up in enumerate(self.layers_up):\n",
        "            if inx == 0:\n",
        "                x = layer_up(x)\n",
        "            else:\n",
        "                x = layer_up(x+skip_list[-inx])\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward_final(self, x):\n",
        "        x = self.final_up(x)\n",
        "        x = x.permute(0,3,1,2)\n",
        "        x = self.final_conv(x)\n",
        "        return x\n",
        "\n",
        "    def forward_backbone(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        if self.ape:\n",
        "            x = x + self.absolute_pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, skip_list = self.forward_features(x)\n",
        "        x = self.forward_features_up(x, skip_list)\n",
        "        x = self.forward_final(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRuLuGZ65zsb",
        "outputId": "a2512a8c-ab10-44b9-dc6e-2ab2fb25e57f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1000, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "a = torch.randn(1,3,224,224).cuda()\n",
        "\n",
        "model = VSSM_V2().cuda()\n",
        "\n",
        "model(a).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxShA5tV5zsb",
        "outputId": "aee221f2-cf83-4b4e-aba9-4d85f17f9a17"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VSSLayer(\n",
              "  (blocks): ModuleList(\n",
              "    (0): VSSBlock(\n",
              "      (ln_1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "      (self_attention): SS2D(\n",
              "        (in_proj): Linear(in_features=96, out_features=384, bias=False)\n",
              "        (conv2d): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
              "        (act): SiLU()\n",
              "        (out_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "        (out_proj): Linear(in_features=192, out_features=96, bias=False)\n",
              "      )\n",
              "      (drop_path): timm.DropPath(0.0)\n",
              "    )\n",
              "    (1): VSSBlock(\n",
              "      (ln_1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "      (self_attention): SS2D(\n",
              "        (in_proj): Linear(in_features=96, out_features=384, bias=False)\n",
              "        (conv2d): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
              "        (act): SiLU()\n",
              "        (out_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "        (out_proj): Linear(in_features=192, out_features=96, bias=False)\n",
              "      )\n",
              "      (drop_path): timm.DropPath(0.0071428571827709675)\n",
              "    )\n",
              "  )\n",
              "  (downsample): PatchMerging2D(\n",
              "    (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
              "    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "model.layers[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWaI8tHQ5zsc",
        "outputId": "c8f63905-3d64-4ebf-b774-491927d74499"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VSSLayer_up_V2(\n",
              "  (blocks): ModuleList(\n",
              "    (0): V2(\n",
              "      (vssm): VSSBlockCustom(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): SS2D(\n",
              "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
              "          (conv2d): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
              "          (act): SiLU()\n",
              "          (out_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
              "        )\n",
              "        (drop_path): timm.DropPath(0.10000000149011612)\n",
              "      )\n",
              "      (conv_bbone): Sequential(\n",
              "        (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
              "        (1): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): SiLU()\n",
              "        (3): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (mlp): Sequential(\n",
              "        (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): SiLU()\n",
              "        (2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): V2(\n",
              "      (vssm): VSSBlockCustom(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): SS2D(\n",
              "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
              "          (conv2d): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
              "          (act): SiLU()\n",
              "          (out_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
              "        )\n",
              "        (drop_path): timm.DropPath(0.09285714477300644)\n",
              "      )\n",
              "      (conv_bbone): Sequential(\n",
              "        (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
              "        (1): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): SiLU()\n",
              "        (3): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (mlp): Sequential(\n",
              "        (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): SiLU()\n",
              "        (2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "model.layers_up[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Window Attention"
      ],
      "metadata": {
        "id": "WriJvsgg8-7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Codes"
      ],
      "metadata": {
        "id": "GbUuiG8O9bsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def window_partition(x, window_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: (B, H, W, C)\n",
        "        window_size (int): window size\n",
        "\n",
        "    Returns:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "    \"\"\"\n",
        "    B, H, W, C = x.shape\n",
        "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
        "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "    return windows\n",
        "\n",
        "\n",
        "def window_reverse(windows, window_size, H, W):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "        window_size (int): Window size\n",
        "        H (int): Height of image\n",
        "        W (int): Width of image\n",
        "\n",
        "    Returns:\n",
        "        x: (B, H, W, C)\n",
        "    \"\"\"\n",
        "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
        "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
        "    return x"
      ],
      "metadata": {
        "id": "8z7H0X0b9rbd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cx2C_c7e5zsc"
      },
      "outputs": [],
      "source": [
        "class WindowAttention(nn.Module):\n",
        "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
        "    It supports both of shifted and non-shifted window.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        window_size (tuple[int]): The height and width of the window.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
        "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
        "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size  # Wh, Ww\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        # define a parameter table of relative position bias\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "        # get pair-wise relative position index for each token inside the window\n",
        "        coords_h = torch.arange(self.window_size[0])\n",
        "        coords_w = torch.arange(self.window_size[1])\n",
        "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
        "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
        "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input features with shape of (num_windows*B, N, C)\n",
        "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
        "        \"\"\"\n",
        "        B_, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))\n",
        "\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, N, N)\n",
        "            attn = self.softmax(attn)\n",
        "        else:\n",
        "            attn = self.softmax(attn)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
        "\n",
        "    def flops(self, N):\n",
        "        # calculate flops for 1 window with token length of N\n",
        "        flops = 0\n",
        "        # qkv = self.qkv(x)\n",
        "        flops += N * self.dim * 3 * self.dim\n",
        "        # attn = (q @ k.transpose(-2, -1))\n",
        "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
        "        #  x = (attn @ v)\n",
        "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
        "        # x = self.proj(x)\n",
        "        flops += N * self.dim * self.dim\n",
        "        return flops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformerBlock(nn.Module):\n",
        "    r\"\"\" Swin Transformer Block.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        input_resolution (tuple[int]): Input resulotion.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        window_size (int): Window size.\n",
        "        shift_size (int): Shift size for SW-MSA.\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
        "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
        "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        if min(self.input_resolution) <= self.window_size:\n",
        "            # if window size is larger than input resolution, we don't partition windows\n",
        "            self.shift_size = 0\n",
        "            self.window_size = min(self.input_resolution)\n",
        "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
        "\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = WindowAttention(\n",
        "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "        if self.shift_size > 0:\n",
        "            # calculate attention mask for SW-MSA\n",
        "            H, W = self.input_resolution\n",
        "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
        "            h_slices = (slice(0, -self.window_size),\n",
        "                        slice(-self.window_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            w_slices = (slice(0, -self.window_size),\n",
        "                        slice(-self.window_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            cnt = 0\n",
        "            for h in h_slices:\n",
        "                for w in w_slices:\n",
        "                    img_mask[:, h, w, :] = cnt\n",
        "                    cnt += 1\n",
        "\n",
        "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
        "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
        "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
        "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
        "        else:\n",
        "            attn_mask = None\n",
        "\n",
        "        self.register_buffer(\"attn_mask\", attn_mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        H, W = self.input_resolution\n",
        "        B, L, C = x.shape\n",
        "        assert L == H * W, \"input feature has wrong size\"\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        # cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            shifted_x = x\n",
        "\n",
        "        # partition windows\n",
        "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
        "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
        "\n",
        "        # W-MSA/SW-MSA\n",
        "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
        "\n",
        "        # merge windows\n",
        "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
        "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
        "\n",
        "        # reverse cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            x = shifted_x\n",
        "        x = x.view(B, H * W, C)\n",
        "\n",
        "        # FFN\n",
        "        x = shortcut + self.drop_path(x)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
        "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        H, W = self.input_resolution\n",
        "        # norm1\n",
        "        flops += self.dim * H * W\n",
        "        # W-MSA/SW-MSA\n",
        "        nW = H * W / self.window_size / self.window_size\n",
        "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
        "        # mlp\n",
        "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
        "        # norm2\n",
        "        flops += self.dim * H * W\n",
        "        return flops"
      ],
      "metadata": {
        "id": "7v6HdXiH9twx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = SwinTransformerBlock(dim=768, input_resolution=(7,7), num_heads=3, window_size=7, shift_size=0)\n",
        "net(torch.randn(1,49,768)).shape"
      ],
      "metadata": {
        "id": "AZhu6hR9_aZS",
        "outputId": "05cbed35-5776-4dc5-86b3-e158013c411b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 49, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x_0ajx7p_uiA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}